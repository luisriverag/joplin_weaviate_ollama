# JOPLIN PRIVATE AI

**RAG your notes locally with 100% openâ€‘source stack**

## What is it?

Pipeline that syncs Joplinâ€‘exported Markdown/PDF/images into a local Weaviate vector DB, embeds with Sentenceâ€‘Transformers, and lets you chat with them using Ollama. Optional Telegram bot provided.

## Highlights

- âš¡ **Fast multithreaded scan & OCR** â€” uses all CPU cores
    
- ðŸ•’ **Contentâ€‘hash caching** â€” uploads only new/changed files
    
- ðŸ” **Perâ€‘image OCR timeout** â€” hung files canâ€™t block a sync (`--timeout`)
    
- ðŸ“Š **Progress bars** â€” see whatâ€™s happening with `--progress`
    
- ðŸ§  **Generic RAG stack** â€” LangChain + Ollama + Weaviate
    
- ðŸ—‚ï¸ **Builtâ€‘in document classifier** â€” prioritises personal docs when answering
    
- ðŸž **Debug tools** â€” inspect retrieved docs or save a JSON classification config
    
- ðŸ“± **Optional Telegram bot** â€” restricted to a single user ID
    
- ðŸ”§ **Fully configurable via** `.env`
    

## Quick start

```
# 1. Clone & install
git clone https://github.com/you/joplin-private-ai.git
cd joplin-private-ai
python -m pip install -r requirements.txt

# 2. System deps
# Ubuntu:
sudo apt install tesseract-ocr
# macOS (brew) / Windows â†’ see Tesseract docs

# Ollama & Docker (Weaviate)
# https://ollama.com/download
docker compose up -d      # starts Weaviate on :8080

# 3. Configure
cp sample.env .env
nano .env                 # point MD_FOLDERS at your Joplin MD export
```

## Sync & upload

```
# full run, 8 threads, progress bars
python joplin_sync.py --sync --upload --workers 8 --progress
# custom OCR timeout and 500â€‘note batches
python joplin_sync.py --sync --upload --timeout 30 --batch-size 500
```

## Chat locally

```
python rag_query.py
ðŸ§  > What is my motorbike's license plate?
```

### CLI tricks

- `debug:<query>` â€“ show top retrieved docs with classifications
    
- `no-analysis:<query>` â€“ skip ownership analysis
    
- `save-config my_config.json` â€“ write current classifier template
    

## Telegram bot (optional)

```
python telegram_rag_bot.py
# then on Telegram
/start
> What is my motorbike's license plate?
```

Bot replies only to the `TELEGRAM_USER_ID` set in `.env`.

## File overview

| File | Purpose |
| --- | --- |
| `joplin_sync.py` | Multithreaded sync/embedding/upload tool |
| `rag_query.py` | Local interactive RAG CLI with classifier & debug |
| `telegram_rag_bot.py` | Singleâ€‘user Telegram interface |
| `sync_and_upload.sh` | Convenience wrapper (legacy) |
| `docker-compose.yml` | Weaviate stack |
| `sample.env` / `.env` | Runtime configuration |
| `note_cache.json` | Autoâ€‘generated dedup cache |

## Environment variables

Important keys (see `.env` for full list):

```
MD_FOLDERS=/path/to/joplin_export1,/path/to/second/folder
WEAVIATE_URL=http://localhost:8080
WEAVIATE_INDEX=JoplinNotes
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
OLLAMA_MODEL=llama3:8b
TELEGRAM_BOT_TOKEN=...
TELEGRAM_USER_ID=123456789
```

## FAQ

**Why not point at my live Joplin profile?** A clean Markdown export avoids syncing inâ€‘progress changes and keeps your original notebooks safe.

**Is everything local?** Yes â€“ unless you enable the Telegram bot, which naturally sends your queries and model answers through Telegramâ€™s servers.

**Can I use another model/database?** Sure. Swap `EMBEDDING_MODEL`, `OLLAMA_MODEL`, or plug in a remote Weaviate URL.

## NEXT STEPS

Looking into Element / Matrix.org to replace Telegram with an open source platform that allows bots and end to end encryption.
