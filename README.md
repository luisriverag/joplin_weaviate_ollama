# JOPLINÂ +Â WEAVIATEÂ +Â OLLAMAÂ = **Joplin PrivateAI**

**ğŸ’¡ 100 % local, privacyâ€‘first KnowledgeÂ Base** â€” Chat with your Joplin vault **offline** and **under your full control**.

* * *

## Project vision

Replace cloud Retrievalâ€‘Augmentedâ€‘Generation services with a **100 % local stack**:

- **No vendor lockâ€‘in** â€“ all components are openâ€‘source, containerised, and easily swappable.
    
- **Respect your data** â€“ nothing leaves your machine unless *you* enable the optional Telegram bridge.
    
- **Optimised for commodity hardware** â€“ runs smoothly on an 8 GB RAM laptop, scales down to a Raspberry PiÂ 4 (via *fast* CLI) and up to a GPU workstation.
    
- **Transparent plumbing** â€“ plain Python, Docker, and YAML; every step is auditable.
    

* * *

## Architecture

```text
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  ingestion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Joplin Export â”‚  Markdown / Attachments / Resources        â”‚
              â”‚           `joplin_sync.py`                 â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚  batches of chunks
                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚      Weaviate         â”‚  â‘¡ retrieval
                        â”‚  (BM25 + vectors)     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                  â–²                             â”‚
                                  â”‚ GraphQL                     â”‚
                                  â”‚ â‘¢ context                  â”‚
                                  â–¼                             â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                        â”‚       Ollama          â”‚  â‘£ answer     â”‚
                        â”‚ (local LLM/adapter)   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–²
                                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Terminal CLI  â”‚  Telegram (opt.)   â”‚  REST APIâ€  (todo)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              Conversation
```

*(â€  a minimal Flask gateway is planned for v2025â€‘Q3.)*

### Data flow

1.  **Ingestion** â€“ `joplin_sync.py` scans export folders, slices Markdown into ~500 token chunks, OCRs images/PDFs, computes embeddings (dimensionÂ 384), and uploads JSON batches.
    
2.  **Storage** â€“ Chunks live in Weaviate with original note path, resource hash, timestamps, and tag list.
    
3.  **Retrieval** â€“ CLIs perform hybrid search (BM25 + cosine) with a configurable Î± blend, rerank hits with recency & ownership heuristics, and craft the final prompt.
    
4.  **Generation** â€“ A local Ollama model (e.g. *llama3:8bâ€‘instruct*) produces the answer, including citations back to note source paths.
    

* * *

## Component deepâ€‘dive

| Name | Highlights | Language / deps |
| --- | --- | --- |
| **`joplin_sync.py`** | Multithreaded crawler (I/O bound) â€¢ OCR via Tesseract â€¢ Incremental cache keyed by SHAâ€‘256 â€¢ Automatic language detection for OCR â€¢ Hybrid schema bootstrapper â€¢ Search playground (`--search`) | PythonÂ 3.10, TesseractÂ â‰¥ 5.0 |
| **`rag_query.py`** | Intent classifier (EN/ES) â€¢ Slidingâ€‘window memory (8Â turns) â€¢ Ownership/procedural/temporal stages â€¢ Structured logs (`structlog`) â€¢ Rich colour console | PythonÂ 3.10, LangChainÂ 0.2, Ollama |
| **`rag_query-fast.py`** | â‰¤ 2 s cold start â€¢ No memory, direct retrieval â€¢ YAML pattern matcher â€¢ Ideal for RaspberryÂ Pi | Same |
| **`docker-compose.yml`** | Readâ€‘only Weaviate vectorizer â€¢ Singleâ€‘node Raft with persistent shards â€¢ Tweaked limits (`QUERY_DEFAULTS_LIMIT=25`) | DockerÂ â‰¥ 20.10 |
| `telegram_rag_bot.py` | Oneâ€‘user hardâ€‘locked â€¢ Markdown rendering â€¢ `/summary`, `/reset` commands | pythonâ€‘telegramâ€‘botÂ 21 |
| `sync_and_upload.sh` | CRONâ€‘friendly wrapper for delta sync & upload | Bash |
| `scripts/migrations/` | Future schema migrations (placeholder) | Python |

* * *

## Requirements

| Category | Minimum | Recommended | Notes |
| --- | --- | --- | --- |
| **OS** | Linux x86â€‘64 / ARMÂ 64, macOS, WindowsÂ 11 WSL2 | UbuntuÂ 22.04 LTS | On macOS, macFUSE may boost FS perf |
| **Python** | 3.9 | 3.10 | Tested with CPython |
| **CPU** | 2Â cores | 4â€‘8Â cores | Heavy OCR benefits |
| **RAM** | 4 GB | 8â€‘16 GB | Embeddings cached in RAM |
| **Storage** | 5 GB | 20 GB+ | Weaviate grows with vault size |
| **GPU** | optional | 8 GBÂ VRAM | Accelerates Ollama (CUDAÂ /Â Metal) |
| **Tesseract** | 5.0 | 5.3 + `tesseract-lang` | Add `eng`, `spa`, `deu`, â€¦ |

* * *

## Installation

### 1 Â· Clone & Python packages

```bash
git clone https://github.com/luisriverag/joplin_weaviate_ollama/.git
cd joplin_weaviate_ollama
python -m venv .venv
source .venv/bin/activate   # Windows â†’ .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2 Â· System packages

- **Debian/Ubuntu**
    
    ```bash
    sudo apt update
    sudo apt install tesseract-ocr libtesseract-dev poppler-utils
    sudo apt install tesseract-ocr-eng tesseract-ocr-spa  # language packs
    ```
    
- **Arch**
    
    ```bash
    sudo pacman -S tesseract tesseract-data-eng tesseract-data-spa
    ```
    
- **macOS (Homebrew)**
    
    ```bash
    brew install tesseract poppler
    brew install tesseract-lang   # add packs as needed
    ```
    
- **Windows**
    
    1.  Install Tesseract and add to `PATH`.
        
    2.  WSL2 users: run Weaviate inside WSL or in a Docker Desktop Linux container.
        

### 3 Â· Optional GPU acceleration

Install CUDAÂ 12Â +Â cuDNNÂ 8 or Apple Metal plugins â†’ restart Ollama. `ollama pull llama3:8b` will autoâ€‘detect GPU and quantise accordingly.

* * *

## Configuration

Copy `.env` and tweak:

```bash
cp sample.env .env
nano .env
```

| Var | Default | Description |
| --- | --- | --- |
| `MD_FOLDERS` | â€“   | Commaâ€‘separated paths to Joplin exports (`.md` + `_resources`) |
| `WEAVIATE_URL` | `http://localhost:8080` | Use `https://` behind a reverse proxy |
| `WEAVIATE_INDEX` | `Documents` | Each export profile can map to a unique index |
| `EMBEDDING_MODEL` | `sentence-transformers/all-MiniLM-L6-v2` | 384â€‘dim embeddings |
| `OLLAMA_MODEL` | `llama3:8b` | Any Ollama label works (`mistral:7b-instruct`, â€¦) |
| `TELEGRAM_BOT_TOKEN` | *(unset)* | Enable bot when set |
| `TELEGRAM_USER_ID` | *(unset)* | Numeric ID accepted by the bot |
| `TZ` | `UTC` | Affects temporal boosts; set to your timezone |
| `LOG_LEVEL` | `INFO` | `DEBUG` prints request bodies & queries |

* * *

## Running WeaviateÂ &Â Ollama

### 1 Â· Boot vector DB

```bash
docker compose up -d
# wait for readiness
until docker compose logs --tail 5 weaviate | grep -q "Startup complete"; do sleep 2; done
```

### 2 Â· Start local LLM

```bash
ollama serve &
ollama pull llama3:8b   # first time only
```

*Tip:* Put `ollama serve` in a systemd user service for autostart.

* * *

## Synchronising notes

### Common recipes

| Goal | Command |
| --- | --- |
| Smoke test (100 notes) | `python joplin_sync.py --sync --upload --test 100 --progress` |
| Full sync (all CPUs) | `python joplin_sync.py --sync --upload --workers $(nproc) --progress` |
| Incremental delta (CRON) | `./sync_and_upload.sh` |
| Reâ€‘OCR only (skip upload) | `python joplin_sync.py --sync --workers 4 --no-upload` |
| Adâ€‘hoc hybrid search | `python joplin_sync.py --search "license plate" --alpha 0.7 --top-k 5` |

### Flag reference (excerpt)

| Flag | Meaning |
| --- | --- |
| `--workers N` | Threads for OCR/embeddings (defaultÂ =Â CPU count) |
| `--batch-size N` | Upload batch size (notes) |
| `--index NAME` | Override target Weaviate class |
| `--timeout N` | Perâ€‘image OCR timeout (sec) |
| `--alpha 0â€‘1` | Weight of vector vs BM25 for hybrid search |
| `--cache-info` | Print cache statistics |
| `--clean-cache [--force]` | Remove orphaned cache entries |

* * *

## Chatting with the CLIs

### Enhanced CLI (`rag_query.py`)

```bash
python rag_query.py
ğŸ§  > Â¿Tengo la factura de la lavadora?
```

| Shortcut | Effect |
| --- | --- |
| `debug:<query>` | Show top docs, classification, scores |
| `no-analysis:<query>` | Skip ownership / procedural heuristics |
| `summary` | 3â€‘line recap of conversation memory |
| `reset` | Clear memory buffer |
| `help` | Quick inâ€‘CLI cheat sheet |

### Lightweight CLI (`rag_query-fast.py`)

Ideal for constrained hardware; same usage minus memory & reranking:

```bash
python rag_query-fast.py --config patterns.yaml
```

*Hotâ€‘reload config:* `:reload` inside CLI reâ€‘reads the YAML without restart.

* * *

## Telegram bot / Element Matrix.org bot

```bash
python telegram_rag_bot.py &
```

1.  Set `TELEGRAM_BOT_TOKEN` and `TELEGRAM_USER_ID` in `.env`.
    
2.  Send `/start` to your bot.
    
3.  Use Markdown or plain text; answers cite note paths like `notebooks/Finanzas/2023-IRPF.md`.
    

*Security note:* The bot rejects messages from any ID except the whitelisted one.

Note: ragbot_elementmatrix.py is untested, pull requests welcome



* * *

## MaintenanceÂ & troubleshooting

| Symptom | Likely cause | Remedy |
| --- | --- | --- |
| `Connection refused` to Weaviate | Container not ready | `docker compose logs -f weaviate` |
| "Phantom shard" error | Dirty shutdown | Add `RAFT_ENABLE_ONE_NODE_RECOVERY: true`, restart once |
| OCR stalls on TIFF | Bad scan | Lower `--timeout`; file skipped & logged |
| Answers too generic | Using *fast* CLI | Switch to full CLI or enlarge `top-k` |
| Model OOM on GPU | VRAM too small | Pull 4â€‘bit quant (`llama3:8b-q4_0`) or use CPU |

Logs:

- `sync_errors.log` â€“ ingestion issues
    
- `~/.ollama/logs` â€“ LLM server
    
- `weaviate-data/logs` â€“ DB warnings
    

* * *

## Performance tuning

| Lever | Default | Faster | Notes |
| --- | --- | --- | --- |
| **Embeddings model** | MiniLMâ€‘L6â€‘v2 | allâ€‘mpnetâ€‘baseâ€‘v2 (768Â dim) | Higher recall, slower |
| **Batch size** | 32 notes | 128 | RAM bound |
| **OCR lang packs** | eng + spa | exact language subset | Fewer dictionaries = faster |
| **`--alpha`** | 0.7 | 0.5 (vectorâ€‘heavy) | LowerÂ â†’ BM25 heavier |
| **LLM quant** | q4_0 | q2_K | Lower VRAM, slower |
| **Weaviate cache** | off | `search.cache` plugâ€‘in | Enterprise feature |

* * *

## Security & privacy

- **Network isolation** â€“ Weaviate listens on `localhost` by default. Use firewall rules or Docker network to restrict.
    
- **Encrypted volume** â€“ Store `weaviate-data/` on an encrypted partition if note content is sensitive.
    
- **No telemetry** â€“ `ENABLE_TELEMETRY=false` in compose file.
    
- **Telegram bridge** â€“ Remember all traffic goes through Telegramâ€™s servers; dont run the telegram ragbot if strict privacy is required.
    

* * *

## FAQ

- **Why two CLIs?** â€” `rag_query.py` aims for maximal context awareness; `rag_query-fast.py` boots in seconds, fitting IoT / Pi devices.
    
- **Does any data leave my machine?** â€” No, unless *you* enable the Telegram bot or point `WEAVIATE_URL` to a remote host.
    
- **How do I wipe and rebuild the index?** â€” `docker compose down -v` to drop shards, delete `note_cache.json`, then run a fresh sync.
    
- **Can I disable OCR?** â€” Yes: `--no-ocr` skips image/PDF text extraction.
    

* * *

## License

MIT
